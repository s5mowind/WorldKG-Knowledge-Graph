{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting haversine\n",
      "  Downloading haversine-2.8.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Installing collected packages: haversine\n",
      "Successfully installed haversine-2.8.0\n"
     ]
    }
   ],
   "source": [
    "#! pip install haversine"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T10:26:25.483771300Z",
     "start_time": "2023-10-04T10:26:23.279855300Z"
    }
   },
   "id": "b0bb8e49cbaee4e3"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygeohash\n",
      "  Downloading pygeohash-1.2.0.tar.gz (5.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: pygeohash\n",
      "  Building wheel for pygeohash (setup.py): started\n",
      "  Building wheel for pygeohash (setup.py): finished with status 'done'\n",
      "  Created wheel for pygeohash: filename=pygeohash-1.2.0-py2.py3-none-any.whl size=6178 sha256=f560e3a3a0ca51e9a40983dcbffbf17cfe1a11d3c22b6eb692f2bbdfe205f0e4\n",
      "  Stored in directory: c:\\users\\morit\\appdata\\local\\pip\\cache\\wheels\\95\\22\\7a\\35719e5f20cdc599cc837c67031a3ec2f011e1d418f57a37ce\n",
      "Successfully built pygeohash\n",
      "Installing collected packages: pygeohash\n",
      "Successfully installed pygeohash-1.2.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install pygeohash"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-04T09:56:11.531507200Z"
    }
   },
   "id": "5184d9286174e156"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pygeohash\n",
    "import re\n",
    "from haversine import haversine\n",
    "import gensim\n",
    "import requests\n",
    "import os\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T07:23:30.215799200Z",
     "start_time": "2023-10-19T07:23:29.504265100Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def wkt_to_geohash(wkt:str) -> str:\n",
    "    m = re.match(r'Point\\((.*) (.*)\\)', wkt)\n",
    "    if m:\n",
    "        lon = float(m.group(1))\n",
    "        lat = float(m.group(2))\n",
    "        return pygeohash.encode(longitude=lon, latitude=lat, precision = 6)\n",
    "    else:\n",
    "        return '000000'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T07:23:30.217795Z",
     "start_time": "2023-10-19T07:23:30.212711Z"
    }
   },
   "id": "3483d84dc1e28e3c"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def haversine_from_geohash(hash1:str, hash2:str) -> float:\n",
    "    \"\"\"\n",
    "    function to estimate haversine distance from geohash strings\n",
    "    :param hash1: first loaction encoded in geohash\n",
    "    :param hash2: second location encoded in geohash\n",
    "    :return: estimated distance between locations in km\n",
    "    \"\"\"\n",
    "    # only take first two parts of tuples, rest are error estimations\n",
    "    hd = haversine(pygeohash.decode_exactly(hash1)[:2], pygeohash.decode_exactly(hash2)[:2])\n",
    "    return hd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T07:23:30.235705200Z",
     "start_time": "2023-10-19T07:23:30.219791400Z"
    }
   },
   "id": "c5fd235825b4a591"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def generate_embedding(sentence:str, model:gensim.models.fasttext) -> np.array:\n",
    "    \"\"\"\n",
    "    generate average embedding for list of strings\n",
    "    :param sentence: string to embed\n",
    "    :param model: model to generate embeddings from\n",
    "    :return: average embedding of all words\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    words = sentence.split(' ')  # splitting to avoid parsing as subwords\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            embeddings.append(model.wv[word])\n",
    "        else:\n",
    "            embeddings.append(np.zeros(300))\n",
    "    return np.mean(embeddings, axis=0).tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T09:38:17.864161Z",
     "start_time": "2023-10-19T09:38:17.808198800Z"
    }
   },
   "id": "4550e410e5d5805b"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "def generate_tail_label_embedding(label:str, name:str, model:gensim.models.fasttext) -> np.array:\n",
    "    \"\"\"\n",
    "    wrapper function to conditionally combine label and name embeddings for tail candidates\n",
    "    :param label: string containing label\n",
    "    :param name: string containing name\n",
    "    :param model: fasttext model used to generate embeddings\n",
    "    :return: conditionally, average embedding of labels \n",
    "    \"\"\"\n",
    "    embedding = generate_embedding(label, model)\n",
    "    if name != '<UNK>':\n",
    "        embedding = np.mean([embedding, generate_embedding(name, model)], axis=0)\n",
    "    return embedding"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T10:06:38.106384500Z",
     "start_time": "2023-10-19T10:06:38.053820900Z"
    }
   },
   "id": "879d469ccb4e9c9a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# need geohash distance based on type\n",
    "# need cosine between literal and embedded labels\n",
    "# need cosine between relation and type"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T07:23:30.640719200Z",
     "start_time": "2023-10-19T07:23:30.611470300Z"
    }
   },
   "id": "3365157adf516bc8"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "location = 'E:\\Datasets/Embeddings'\n",
    "if os.path.exists(os.path.join(location, 'cc.en.300.bin.gz')):\n",
    "    print('file present')\n",
    "else:\n",
    "    r = requests.get('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz')\n",
    "    with open(os.path.join(location, 'cc.en.300.bin.gz'), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "ft_model = gensim.models.fasttext.load_facebook_model(os.path.join(location, 'cc.en.300.bin.gz'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T08:30:38.632912900Z",
     "start_time": "2023-10-19T08:24:00.929496300Z"
    }
   },
   "id": "6e3df7fe815e2db0"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "candidates = pd.read_parquet('candidates_type.parquet.gzip')\n",
    "col_names = list(candidates.columns) + ['geoh'] + [f'label_emb{i}' for i in range(300)]\n",
    "geoh = candidates.apply(lambda row: wkt_to_geohash(row['location']), axis=1)\n",
    "label_emb = candidates.apply(lambda row: generate_tail_label_embedding(row['label'], row['label_en'], ft_model), axis=1, result_type='expand')\n",
    "candidates = pd.concat([candidates, geoh, label_emb], axis=1)\n",
    "candidates.columns = col_names\n",
    "type_map = {obj_type: np.zeros if obj_type == '<UNK>' else generate_embedding(obj_type, ft_model) for obj_type in candidates['type'].unique()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T10:10:38.529868900Z",
     "start_time": "2023-10-19T10:10:27.367828700Z"
    }
   },
   "id": "aad862fb2608e302"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "candidates.to_parquet('candidates_embedding.parquet.gzip', compression='gzip', engine='pyarrow')\n",
    "with open('type_map.json', 'w') as f:\n",
    "    json.dump(type_map, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T10:15:24.911857300Z",
     "start_time": "2023-10-19T10:15:13.157130300Z"
    }
   },
   "id": "b7a71c1346e90c5e"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "subjects = pd.read_parquet('subjects_type.parquet.gzip')\n",
    "subjects['geohash'] = subjects.apply(lambda row: wkt_to_geohash(row['location']), axis=1)\n",
    "predicate_map = {pred: generate_embedding(pred.split(':')[-1], ft_model) for pred in subjects['predicate'].unique()}\n",
    "literal_map = {literal: generate_embedding(literal, ft_model) for literal in subjects['literal'].unique()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T09:53:27.063858600Z",
     "start_time": "2023-10-19T09:53:25.383012100Z"
    }
   },
   "id": "9e5a7dba7aac946f"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "subjects.to_parquet('subjects_embedding.parquet.gzip', compression='gzip', engine='pyarrow')\n",
    "with open('predicate_map.json', 'w') as f:\n",
    "    json.dump(predicate_map, f)\n",
    "with open('literal_map.json', 'w') as f:\n",
    "    json.dump(literal_map, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T10:15:27.612106200Z",
     "start_time": "2023-10-19T10:15:27.235781800Z"
    }
   },
   "id": "93ac0383c4bb7759"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d115fa357f3a5806"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
